# -*- coding: utf-8 -*-
"""Solucion_caso_practico_unidad_2_IEP_IAA_DL_u2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18xljJBxAmVAc03n_CPL5DDsLmRrXlAQ5

**Paso 1: Cargar y preprocesar CIFAR-100**

Usaremos TensorFlow/Keras y un modelo CNN con máximo 6 capas ocultas.
"""

import tensorflow as tf
from tensorflow.keras.utils import to_categorical
import numpy as np
import pickle

# Cargar los archivos de Google Drive
with open("/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u2/train", 'rb') as f:
    train_dict = pickle.load(f, encoding='bytes')
with open("/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u2/test", 'rb') as f:
    test_dict = pickle.load(f, encoding='bytes')

# Obtener datos y etiquetas
X_train = np.array(train_dict[b'data']).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0
y_train = to_categorical(np.array(train_dict[b'fine_labels']), num_classes=100)

X_test = np.array(test_dict[b'data']).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1) / 255.0
y_test = to_categorical(np.array(test_dict[b'fine_labels']), num_classes=100)

print(f"Train: {X_train.shape}, Test: {X_test.shape}")

"""**Paso 2: Definir CNN con máximo 6 capas ocultas**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(100, activation='softmax')  # 100 clases
])

model.summary()

"""**Paso 3: Compilar y entrenar**"""

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=64,
    validation_split=0.1
)

"""**Implementacion de GANS**"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate, Embedding, multiply, LeakyReLU
from tensorflow.keras.optimizers import Adam

# Cargar Fashion MNIST
(X_train, y_train), (_, _) = fashion_mnist.load_data()
X_train = (X_train.astype(np.float32) - 127.5) / 127.5  # Normalizar a [-1, 1]
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
num_classes = 10
img_shape = (28, 28, 1)
latent_dim = 100

def build_generator():
    noise = Input(shape=(latent_dim,))
    label = Input(shape=(1,), dtype='int32')

    label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))
    model_input = multiply([noise, label_embedding])

    x = Dense(256)(model_input)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dense(512)(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dense(1024)(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dense(np.prod(img_shape), activation='tanh')(x)
    img = Reshape(img_shape)(x)

    return Model([noise, label], img)

def build_discriminator():
    img = Input(shape=img_shape)
    label = Input(shape=(1,), dtype='int32')

    label_embedding = Flatten()(Embedding(num_classes, np.prod(img_shape))(label))
    flat_img = Flatten()(img)

    model_input = multiply([flat_img, label_embedding])

    x = Dense(512)(model_input)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dense(512)(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Dense(1, activation='sigmoid')(x)

    return Model([img, label], x)

# Crear modelos
optimizer = Adam(0.0002, 0.5)

discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

generator = build_generator()

# Red combinada (freeze discriminador durante entrenamiento del generador)
z = Input(shape=(latent_dim,))
label = Input(shape=(1,))
img = generator([z, label])
discriminator.trainable = False
valid = discriminator([img, label])

combined = Model([z, label], valid)
combined.compile(loss='binary_crossentropy', optimizer=optimizer)

def train(epochs, batch_size=128, sample_interval=1000):
    half_batch = batch_size // 2

    for epoch in range(epochs):
        # Entrenar el discriminador
        idx = np.random.randint(0, X_train.shape[0], half_batch)
        imgs, labels = X_train[idx], y_train[idx]

        noise = np.random.normal(0, 1, (half_batch, latent_dim))
        gen_imgs = generator.predict([noise, labels])

        d_loss_real = discriminator.train_on_batch([imgs, labels], np.ones((half_batch, 1)))
        d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], np.zeros((half_batch, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Entrenar el generador
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        sampled_labels = np.random.randint(0, num_classes, batch_size)

        g_loss = combined.train_on_batch([noise, sampled_labels], np.ones((batch_size, 1)))

        # Mostrar progreso
        if epoch % sample_interval == 0:
            print(f"{epoch} [D loss: {d_loss[0]} - acc.: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]")
            sample_images(epoch)

def sample_images(epoch, rows=2, cols=5):
    noise = np.random.normal(0, 1, (rows * cols, latent_dim))
    labels = np.array([num for _ in range(rows) for num in range(cols)])
    gen_imgs = generator.predict([noise, labels])

    gen_imgs = 0.5 * gen_imgs + 0.5

    fig, axs = plt.subplots(rows, cols, figsize=(10, 4))
    cnt = 0
    for i in range(rows):
        for j in range(cols):
            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')
            axs[i, j].set_title(f"Clase: {labels[cnt]}")
            axs[i, j].axis('off')
            cnt += 1
    plt.show()

train(epochs=10000, batch_size=64, sample_interval=1000)

# CONTINUACIÓN DEL ENTRENAMIENTO DE CGAN

import numpy as np

# Asegúrate de tener definidas estas variables antes de correr:
# generator, discriminator, combined, X_train, y_train, num_classes, latent_dim

# ⚠️ y_train debe estar en formato de etiquetas enteras, NO one-hot
# Si tu y_train es one-hot, conviértelo con:
if y_train.ndim > 1:
    y_train = np.argmax(y_train, axis=1)

# Parámetros
start_epoch = 10000
end_epoch = 20000
batch_size = 64
sample_interval = 1000

# Etiquetas para reales y falsas
valid = np.ones((batch_size, 1))
fake = np.zeros((batch_size, 1))

# Entrenamiento CGAN
for epoch in range(start_epoch, end_epoch):
    # -------- Entrenar Discriminador --------
    idx = np.random.randint(0, X_train.shape[0], batch_size)
    imgs = X_train[idx]
    labels = y_train[idx].reshape(-1, 1).astype(np.int32)  # ✅ CORRECTO: (batch_size, 1), int32

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    gen_imgs = generator.predict([noise, labels], verbose=0)

    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)
    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # -------- Entrenar Generador --------
    sampled_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1).astype(np.int32)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))

    g_loss = combined.train_on_batch([noise, sampled_labels], valid)

    # Mostrar progreso
    if epoch % 100 == 0:
        print(f"{epoch} [D loss: {d_loss[0]:.4f} - acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

    # Guardar imágenes generadas
    if epoch % sample_interval == 0:
        sample_images(epoch)

from keras.models import Model
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate, Embedding, multiply, Conv2DTranspose, Conv2D, LeakyReLU, BatchNormalization
from keras.optimizers import Adam

# Parámetros
latent_dim = 100
img_shape = (28, 28, 1)
num_classes = 10

# Generador
def build_generator():
    noise = Input(shape=(latent_dim,))
    label = Input(shape=(1,), dtype='int32')

    # Convertir etiquetas en one-hot
    label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))
    model_input = concatenate([noise, label_embedding])

    x = Dense(128 * 7 * 7)(model_input)
    x = LeakyReLU(0.2)(x)
    x = Reshape((7, 7, 128))(x)
    x = Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    x = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    img = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(x)

    return Model([noise, label], img)

# Discriminador
def build_discriminator():
    img = Input(shape=img_shape)
    label = Input(shape=(1,), dtype='int32')

    label_embedding = Flatten()(Embedding(num_classes, np.prod(img_shape))(label))
    label_reshaped = Reshape(img_shape)(label_embedding)

    model_input = concatenate([img, label_reshaped])

    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(model_input)
    x = LeakyReLU(0.2)(x)
    x = Dropout(0.3)(x)
    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)
    x = LeakyReLU(0.2)(x)
    x = Dropout(0.3)(x)
    x = Flatten()(x)
    validity = Dense(1, activation='sigmoid')(x)

    return Model([img, label], validity)

# Crear y compilar discriminador
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy',
                      optimizer=Adam(0.0002, 0.5),
                      metrics=['accuracy'])

# Crear generador
generator = build_generator()

# El generador se entrena en el contexto del modelo combinado
noise = Input(shape=(latent_dim,))
label = Input(shape=(1,))
img = generator([noise, label])
discriminator.trainable = False
valid = discriminator([img, label])

# Modelo combinado
combined = Model([noise, label], valid)
combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

# ⚠️ Asegúrate de haber definido: generator, discriminator, combined, all_clothes, all_labels, latent_dim, num_classes

# Convertir etiquetas a enteros si están en one-hot
if all_labels.ndim > 1:
    all_labels = np.argmax(all_labels, axis=1)

# Parámetros
start_epoch = 20000
end_epoch = 30000
batch_size = 64
sample_interval = 1000

# Etiquetas para imágenes reales y falsas
valid = np.ones((batch_size, 1))
fake = np.zeros((batch_size, 1))

import matplotlib.pyplot as plt
import os

def sample_images(epoch, n_rows=2, n_cols=5):
    """Genera y guarda un conjunto de imágenes del generador condicional"""
    os.makedirs("images", exist_ok=True)

    noise = np.random.normal(0, 1, (n_rows * n_cols, latent_dim))
    labels = np.array([num for _ in range(n_rows) for num in range(n_cols)]).reshape(-1, 1)

    gen_imgs = generator.predict([noise, labels], verbose=0)
    gen_imgs = gen_imgs * 255.0
    gen_imgs = gen_imgs.astype(np.uint8)

    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))
    count = 0
    for i in range(n_rows):
        for j in range(n_cols):
            axs[i, j].imshow(gen_imgs[count, :, :, 0], cmap='gray')
            axs[i, j].set_title(f"Clase: {labels[count][0]}")
            axs[i, j].axis('off')
            count += 1

    fig.suptitle(f"Epoch {epoch}", fontsize=16)
    fig.tight_layout()
    fig.subplots_adjust(top=0.85)
    fig.savefig(f"images/epoch_{epoch}.png")
    plt.close()

# Entrenamiento
for epoch in range(start_epoch, end_epoch):
    # --- Entrenamiento del Discriminador ---
    idx = np.random.randint(0, all_clothes.shape[0], batch_size)
    imgs = all_clothes[idx]
    labels = all_labels[idx].reshape(-1, 1).astype(np.int32)

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    gen_imgs = generator.predict([noise, labels], verbose=0)

    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)
    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # --- Entrenamiento del Generador ---
    sampled_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1).astype(np.int32)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = combined.train_on_batch([noise, sampled_labels], valid)

    # Mostrar progreso
    if epoch % 100 == 0:
        print(f"{epoch} [D loss: {d_loss[0]:.4f} - acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

    # Guardar imágenes generadas
    if epoch % sample_interval == 0:
        sample_images(epoch)

from keras.models import Model
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Embedding, Concatenate
from keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, Conv2D, Conv2DTranspose
from keras.optimizers import Adam
import numpy as np
import tensorflow as tf

# Hiperparámetros
latent_dim = 100
num_classes = 10
image_shape = (28, 28, 1)

# Generador
noise_input = Input(shape=(latent_dim,))
label_input = Input(shape=(1,), dtype='int32')
label_embedding = Embedding(num_classes, latent_dim)(label_input)
label_embedding = Reshape((latent_dim,))(label_embedding)
model_input = tf.keras.layers.Multiply()([noise_input, label_embedding])

x = Dense(128 * 7 * 7, activation="relu")(model_input)
x = Reshape((7, 7, 128))(x)
x = BatchNormalization(momentum=0.8)(x)
x = Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)
x = LeakyReLU(alpha=0.2)(x)
x = BatchNormalization(momentum=0.8)(x)
x = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)
x = LeakyReLU(alpha=0.2)(x)
x = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(x)

generator = Model([noise_input, label_input], x)

# Discriminador
img_input = Input(shape=image_shape)
label_input = Input(shape=(1,), dtype='int32')
label_embedding = Embedding(num_classes, np.prod(image_shape))(label_input)
label_embedding = Reshape(image_shape)(label_embedding)

combined_input = Concatenate(axis=-1)([img_input, label_embedding])

x = Conv2D(64, kernel_size=3, strides=2, padding='same')(combined_input)
x = LeakyReLU(alpha=0.2)(x)
x = Dropout(0.4)(x)
x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)
x = LeakyReLU(alpha=0.2)(x)
x = Dropout(0.4)(x)
x = Flatten()(x)
x = Dense(1, activation='sigmoid')(x)

discriminator = Model([img_input, label_input], x)
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002), metrics=['accuracy'])

# Modelo combinado (generador + discriminador)
noise_input = Input(shape=(latent_dim,))
label_input = Input(shape=(1,), dtype='int32')
img = generator([noise_input, label_input])
discriminator.trainable = False
validity = discriminator([img, label_input])

combined = Model([noise_input, label_input], validity)
combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002))

print("✅ Modelos redefinidos. Ahora puedes guardar:")

generator.save("generator_25700.keras")
discriminator.save("discriminator_25700.keras")
combined.save("combined_25700.keras")

import matplotlib.pyplot as plt
from keras.utils import to_categorical

labels = np.array([i for i in range(num_classes)])
labels_input = labels.reshape(-1, 1).astype(np.int32)
noise = np.random.normal(0, 1, (num_classes, latent_dim))

# Generar imágenes
gen_imgs = generator.predict([noise, labels_input])

# Mostrar
for i in range(num_classes):
    plt.imshow(gen_imgs[i].squeeze(), cmap="gray")
    plt.title(f"Etiqueta: {labels[i]}")
    plt.axis("off")
    plt.show()

from keras.datasets import fashion_mnist
import numpy as np

# Cargar datos
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
all_clothes = np.concatenate([x_train, x_test])
all_labels = np.concatenate([y_train, y_test])

# Normalizar y dar forma
all_clothes = all_clothes.astype("float32") / 255.0
all_clothes = np.reshape(all_clothes, (-1, 28, 28, 1))

# Convertir etiquetas a enteros (por si ya estaban one-hot)
if all_labels.ndim > 1:
    all_labels = np.argmax(all_labels, axis=1)

print("✅ Datos cargados y normalizados.")

# Asegúrate de tener definidos:
# generator, discriminator, combined, all_clothes, all_labels, latent_dim, num_classes

# Convertir etiquetas a enteros si están one-hot
if all_labels.ndim > 1:
    all_labels = np.argmax(all_labels, axis=1)

# Parámetros de continuación
start_epoch = 25700
end_epoch = 32000
batch_size = 64
sample_interval = 1000

# Etiquetas reales/falsas para el discriminador
valid = np.ones((batch_size, 1))
fake = np.zeros((batch_size, 1))

# Entrenamiento
for epoch in range(start_epoch, end_epoch):
    # 1. Entrenar discriminador
    idx = np.random.randint(0, all_clothes.shape[0], batch_size)
    imgs = all_clothes[idx]
    labels = all_labels[idx].reshape(-1, 1).astype(np.int32)

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    gen_imgs = generator.predict([noise, labels], verbose=0)

    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)
    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # 2. Entrenar generador
    sampled_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1).astype(np.int32)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = combined.train_on_batch([noise, sampled_labels], valid)

    # Mostrar progreso
    if epoch % 100 == 0:
        print(f"{epoch} [D loss: {d_loss[0]:.4f} - acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

    # Generar imagen de muestra
    if epoch % sample_interval == 0:
        # Generar 1 imagen con una etiqueta aleatoria
        test_label = np.random.randint(0, num_classes, 1)
        noise_test = np.random.normal(0, 1, (1, latent_dim))
        gen = generator.predict([noise_test, test_label], verbose=0)
        plt.imshow(gen[0, :, :, 0], cmap='gray')
        plt.title(f"Etiqueta: {test_label[0]}")
        plt.axis('off')
        plt.show()

with open("training_log_25000_32000.txt", "a") as log_file:
    log_file.write(f"{epoch} [D loss: {d_loss[0]:.4f} - acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\n")

from google.colab import drive
drive.mount('/content/drive')

import shutil
import os

# Ruta destino en tu Google Drive
drive_path = "/content/drive/MyDrive/CGAN_Entrenamiento/"

# Crear carpeta si no existe
os.makedirs(drive_path, exist_ok=True)

# Lista de archivos que deseas mover
files_to_move = [
    "/content/combined_25700.keras",
    "/content/discriminator_25700.keras",
    "/content/generator_25700.keras",
    "/content/training_log_25000_32000.txt"
]

# Mover archivos
for file in files_to_move:
    shutil.move(file, drive_path + os.path.basename(file))

print("✅ Archivos movidos a tu Drive:", drive_path)

# 1️⃣ Cargar datos Fashion MNIST y preprocesar
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
import numpy as np

# Cargar Fashion MNIST
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
all_clothes = np.concatenate([x_train, x_test])
all_labels = np.concatenate([y_train, y_test])

# Normalizar imágenes a [0, 1] y redimensionar
all_clothes = all_clothes.astype("float32") / 255.0
all_clothes = np.reshape(all_clothes, (-1, 28, 28, 1))

from keras.models import load_model
from keras.layers import Input
from keras.models import Model
import keras
import numpy as np

# Hiperparámetros
latent_dim = 100
num_classes = 10

# 1. Cargar los modelos entrenados
generator = load_model("/content/drive/MyDrive/CGAN_Entrenamiento/generator_32000.keras")
discriminator = load_model("/content/drive/MyDrive/CGAN_Entrenamiento/discriminator_32000.keras")

# 2. REACTIVAR el entrenamiento para el discriminador
discriminator.trainable = True
d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)
discriminator.compile(loss='binary_crossentropy', optimizer=d_optimizer, metrics=['accuracy'])

# 3. CONSTRUIR de nuevo el modelo combinado (generator + frozen discriminator)
noise_input = Input(shape=(latent_dim,))
label_input = Input(shape=(1,), dtype='int32')
gen_img = generator([noise_input, label_input])

# ⚠️ Congelar el discriminador antes de usarlo en el modelo combinado
discriminator.trainable = False
validity = discriminator([gen_img, label_input])
combined = Model([noise_input, label_input], validity)

# 4. Compilar el modelo combinado
g_optimizer = keras.optimizers.Adam(learning_rate=0.0001)  # más bajo para evitar colapso
combined.compile(loss='binary_crossentropy', optimizer=g_optimizer)

print("✅ Modelos listos. Puedes continuar el entrenamiento desde epoch 32000.")

# Asegúrate de tener cargados: all_clothes, all_labels
# Si es necesario vuelve a cargar el dataset preprocesado

import numpy as np

# Convertir etiquetas one-hot a enteros si es necesario
if all_labels.ndim > 1:
    all_labels = np.argmax(all_labels, axis=1)

# Parámetros de entrenamiento
start_epoch = 32000
end_epoch = 42000
batch_size = 64
sample_interval = 1000

# Entrenamiento CGAN
for epoch in range(start_epoch, end_epoch):
    # === Entrenamiento del Discriminador ===
    idx = np.random.randint(0, all_clothes.shape[0], batch_size)
    imgs = all_clothes[idx]
    labels = all_labels[idx].reshape(-1, 1).astype(np.int32)

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    gen_imgs = generator.predict([noise, labels], verbose=0)

    # Label smoothing y ruido
    valid = np.ones((batch_size, 1)) - np.random.uniform(0, 0.1, (batch_size, 1))
    fake = np.random.uniform(0, 0.1, (batch_size, 1))

    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)
    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # === Entrenamiento del Generador ===
    sampled_labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1).astype(np.int32)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = combined.train_on_batch([noise, sampled_labels], valid)

    # Mostrar progreso
    if epoch % 100 == 0:
        print(f"{epoch} [D loss: {d_loss[0]:.4f} - acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

    # Guardar cada 1000 epochs
    if epoch % sample_interval == 0:
        generator.save(f"/content/drive/MyDrive/CGAN_Entrenamiento/generator_{epoch}.keras")
        discriminator.save(f"/content/drive/MyDrive/CGAN_Entrenamiento/discriminator_{epoch}.keras")
        combined.save(f"/content/drive/MyDrive/CGAN_Entrenamiento/combined_{epoch}.keras")
        with open("/content/drive/MyDrive/CGAN_Entrenamiento/training_log_32000_42000.txt", "a") as log:
            log.write(f"{epoch} [D loss: {d_loss[0]:.4f} - acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\n")
        print(f"✔️ Modelos guardados y log actualizado en epoch {epoch}")

import matplotlib.pyplot as plt
import numpy as np

# Etiquetas a generar
labels = np.array([i for i in range(num_classes)]).reshape(-1, 1)
n_samples = labels.shape[0]

# Ruido
noise = np.random.normal(0, 1, (n_samples, latent_dim))

# Imágenes generadas
gen_imgs = generator.predict([noise, labels], verbose=0)

# Visualización
for i in range(n_samples):
    plt.imshow(gen_imgs[i, :, :, 0], cmap='gray')
    plt.title(f"Clase: {labels[i][0]}")
    plt.axis('off')
    plt.show()

"""Hasta aquí concluimos nuestro modelo manual.

**Para efectos de aprendizaje optamos por usar el método del profesor a continuacion**
"""

# Instalación de librerías necesarias
!pip install -q git+https://github.com/tensorflow/docs
!pip install --upgrade keras

# Importaciones
import keras
from keras import layers
from keras import ops
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Hiperparámetros
latent_dim = 100
num_classes = 10
image_size = 28
batch_size = 64

# Cargar y preprocesar Fashion MNIST
(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()
all_clothes = np.concatenate([x_train, x_test])
all_labels = np.concatenate([y_train, y_test])

# Normalización a [0, 1] y reshape a 28x28x1
all_clothes = all_clothes.astype("float32") / 255.0
all_clothes = np.reshape(all_clothes, (-1, 28, 28, 1))
all_labels = keras.utils.to_categorical(all_labels, num_classes)

# Crear dataset
dataset = tf.data.Dataset.from_tensor_slices((all_clothes, all_labels))
dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)

# Generador
generator = keras.Sequential([
    layers.Input(shape=(latent_dim + num_classes,)),
    layers.Dense(128 * 7 * 7),
    layers.LeakyReLU(0.2),
    layers.Reshape((7, 7, 128)),
    layers.Conv2DTranspose(128, 4, strides=2, padding="same"),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),
    layers.Conv2DTranspose(64, 4, strides=2, padding="same"),
    layers.BatchNormalization(),
    layers.LeakyReLU(0.2),
    layers.Conv2D(1, 7, padding="same", activation="sigmoid")
], name="generator")

# Discriminador
discriminator = keras.Sequential([
    layers.Input(shape=(image_size, image_size, 1 + num_classes)),
    layers.Conv2D(64, 3, strides=2, padding="same"),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),
    layers.Conv2D(128, 3, strides=2, padding="same"),
    layers.LeakyReLU(0.2),
    layers.Dropout(0.3),
    layers.Flatten(),
    layers.Dense(1, activation="sigmoid")
], name="discriminator")

# Clase personalizada de CGAN
class ConditionalGAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super().__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.seed_generator = keras.random.SeedGenerator(1337)
        self.gen_loss_tracker = keras.metrics.Mean(name="generator_loss")
        self.disc_loss_tracker = keras.metrics.Mean(name="discriminator_loss")

    @property
    def metrics(self):
        return [self.gen_loss_tracker, self.disc_loss_tracker]

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super().compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, data):
        real_images, one_hot_labels = data

        # Expandir etiquetas para concatenar como canal extra
        image_one_hot_labels = one_hot_labels[:, :, None, None]
        image_one_hot_labels = ops.repeat(image_one_hot_labels, [image_size * image_size])
        image_one_hot_labels = ops.reshape(image_one_hot_labels, (-1, image_size, image_size, num_classes))

        # Entrada para generador
        batch_size = tf.shape(real_images)[0]
        random_latent_vectors = keras.random.normal((batch_size, self.latent_dim), seed=self.seed_generator)
        random_vector_labels = ops.concatenate([random_latent_vectors, one_hot_labels], axis=1)

        generated_images = self.generator(random_vector_labels)

        # Entrada combinada para discriminador
        fake_input = ops.concatenate([generated_images, image_one_hot_labels], -1)
        real_input = ops.concatenate([real_images, image_one_hot_labels], -1)
        combined_images = ops.concatenate([fake_input, real_input], axis=0)

        # Etiquetas para real/fake
        labels = ops.concatenate([
            ops.ones((batch_size, 1)),
            ops.zeros((batch_size, 1))
        ], axis=0)

        # Entrenar discriminador
        with tf.GradientTape() as tape:
            predictions = self.discriminator(combined_images)
            d_loss = self.loss_fn(labels, predictions)
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))

        # Entrenar generador
        misleading_labels = ops.zeros((batch_size, 1))
        with tf.GradientTape() as tape:
            noise = keras.random.normal((batch_size, self.latent_dim), seed=self.seed_generator)
            input_vector = ops.concatenate([noise, one_hot_labels], axis=1)
            generated_images = self.generator(input_vector)
            fake_input = ops.concatenate([generated_images, image_one_hot_labels], -1)
            predictions = self.discriminator(fake_input)
            g_loss = self.loss_fn(misleading_labels, predictions)
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))

        self.gen_loss_tracker.update_state(g_loss)
        self.disc_loss_tracker.update_state(d_loss)
        return {"g_loss": self.gen_loss_tracker.result(), "d_loss": self.disc_loss_tracker.result()}

# Instancia y compilación
cond_gan = ConditionalGAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)
cond_gan.compile(
    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),
    loss_fn=keras.losses.BinaryCrossentropy()
)

# Entrenamiento
cond_gan.fit(dataset, epochs=22)

# Guardar el modelo completo al finalizar entrenamiento
cond_gan.save("/content/cond_gan_model.keras")
print("Modelo CGAN guardado exitosamente en: /content/cond_gan_model.keras")

# Etiquetas que deseas generar (puedes cambiarlas libremente)
labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
n_samples = len(labels)

# Convertir etiquetas a one-hot
labels_onehot = keras.utils.to_categorical(labels, num_classes)

# Ruido aleatorio para el generador
noise = keras.random.normal(shape=(n_samples, latent_dim))
noise = ops.reshape(noise, (n_samples, latent_dim))

# Concatenar ruido + etiquetas para formar la entrada del generador
noise_and_labels = ops.concatenate([noise, labels_onehot], axis=1)

# Generar imágenes
generated_images = cond_gan.generator.predict(noise_and_labels)

# Convertir las imágenes de [0,1] a [0,255] y visualizar
generated_images *= 255.0
generated_images = generated_images.astype(np.uint8)

# Mostrar imágenes generadas
for i in range(n_samples):
    plt.imshow(generated_images[i, :, :, 0], cmap="gray")
    plt.title(f"Clase: {labels[i]}")
    plt.axis("off")
    plt.show()

print(np.min(generated_images), np.max(generated_images))

from keras.models import load_model
import keras

# 1. Decoramos la clase completa
@keras.saving.register_keras_serializable()
class ConditionalGAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super().__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.seed_generator = keras.random.SeedGenerator(1337)
        self.gen_loss_tracker = keras.metrics.Mean(name="generator_loss")
        self.disc_loss_tracker = keras.metrics.Mean(name="discriminator_loss")

    @property
    def metrics(self):
        return [self.gen_loss_tracker, self.disc_loss_tracker]

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super().compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, data):
        real_images, one_hot_labels = data
        image_size = 28
        num_classes = 10

        image_one_hot_labels = one_hot_labels[:, :, None, None]
        image_one_hot_labels = keras.ops.repeat(image_one_hot_labels, [image_size * image_size])
        image_one_hot_labels = keras.ops.reshape(image_one_hot_labels, (-1, image_size, image_size, num_classes))

        batch_size = tf.shape(real_images)[0]
        random_latent_vectors = keras.random.normal((batch_size, self.latent_dim), seed=self.seed_generator)
        random_vector_labels = keras.ops.concatenate([random_latent_vectors, one_hot_labels], axis=1)

        generated_images = self.generator(random_vector_labels)
        fake_input = keras.ops.concatenate([generated_images, image_one_hot_labels], -1)
        real_input = keras.ops.concatenate([real_images, image_one_hot_labels], -1)
        combined_images = keras.ops.concatenate([fake_input, real_input], axis=0)

        labels = keras.ops.concatenate([
            keras.ops.ones((batch_size, 1)),
            keras.ops.zeros((batch_size, 1))
        ], axis=0)

        with tf.GradientTape() as tape:
            predictions = self.discriminator(combined_images)
            d_loss = self.loss_fn(labels, predictions)
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))

        misleading_labels = keras.ops.zeros((batch_size, 1))
        with tf.GradientTape() as tape:
            noise = keras.random.normal((batch_size, self.latent_dim), seed=self.seed_generator)
            input_vector = keras.ops.concatenate([noise, one_hot_labels], axis=1)
            generated_images = self.generator(input_vector)
            fake_input = keras.ops.concatenate([generated_images, image_one_hot_labels], -1)
            predictions = self.discriminator(fake_input)
            g_loss = self.loss_fn(misleading_labels, predictions)
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))

        self.gen_loss_tracker.update_state(g_loss)
        self.disc_loss_tracker.update_state(d_loss)
        return {"g_loss": self.gen_loss_tracker.result(), "d_loss": self.disc_loss_tracker.result()}

"""**Transición de CGAN Manual a Modelo Subclassing: Lecciones y Decisión Final**

Durante las primeras 20,000 épocas, entrené una CGAN construida manualmente utilizando la API funcional de Keras. Aunque esta versión permitió generar algunas imágenes condicionales, los resultados visuales eran limitados: las formas eran poco definidas, con patrones borrosos o incoherentes. Además, la gestión manual de tensores, las múltiples entradas, y errores recurrentes de forma (ValueError: as_list() is not defined) dificultaban la continuidad del entrenamiento y su depuración.

Por este motivo, decidí adoptar el enfoque propuesto por el profesor basado en Model Subclassing de Keras, que proporciona una arquitectura modular, estructurada y profesional para modelos CGAN. Este nuevo diseño ofrecía varias ventajas clave:

1. Entrenamiento estructurado mediante la implementación de train_step(), lo cual permite gestionar de manera explícita el flujo de entrenamiento por lotes.

2. Integración condicional robusta: las etiquetas se concatenan al ruido para el generador y se añaden como canales en la entrada del discriminador, lo que facilita la generación de imágenes específicas por clase.

3. Métricas internas y compatibilidad con callbacks (como checkpoints, early stopping) para un control fino y seguimiento del entrenamiento.

4. Escalabilidad: facilita la futura incorporación de mejoras como técnicas de regularización o incremento en la resolución.

Uso eficiente de tf.data.Dataset, mejorando el rendimiento en GPU gracias al procesamiento por lotes, embarajado y prefetching.

Sin embargo, tras implementar y entrenar la CGAN con subclassing, los resultados visuales obtenidos no reflejaron mejoras significativas y, en algunos casos, se observaron outputs homogéneos, pixelados o completamente negros. A pesar de las bondades estructurales, el entrenamiento parecía estancarse en un modo colapso.

**Evolución del Modelo CGAN** (17 horas aproximadamente de validacion entre los dos metodos)

**Imagen 1:** descarga (1).png

Epoch inicial (~22K aprox.)

La imagen presenta muchos tonos de gris y una distribución de píxeles más suave.

Aunque todavía no se distinguen formas claras, hay una mayor variedad tonal, lo que indica un generador más activo y menos colapsado.

**Imagen 2:** descarga (2).png
Epoch intermedia (~25K - 32K aprox.)

El contraste ha aumentado y los píxeles se han tornado más binarios (negro/blanco), reduciendo la variabilidad tonal.

El patrón parece aleatorio, pérdida de coherencia estructural. Esto puede ser una señal de modo colapso parcial.

**Imagen 3:** descarga (3).png
Epoch más reciente (~40K)

Claramente binaria, casi sin grises. Parece que el generador está "atascado" en patrones repetitivos.

La estructura es mínima: no hay indicios de formas asociadas a las clases de Fashion MNIST.

Esto confirma que el modelo está colapsado, generando ruido sin aprendizaje significativo nuevo.

**Causas de Pérdida de Eficiencia (Modo Colapso)**

1. Dominancia del Discriminador

  A medida que el discriminador mejora (lo cual es evidente por su acc ≈ 0.00% en los últimos epochs, lo que paradójicamente sugiere que "siempre gana"), el generador deja de recibir gradientes útiles.

2. Learning rate constante

  Aunque ajustaste el learning rate a 0.0001 para el generador, parece que no fue suficiente para romper la zona de confort del generador.

3. Falta de diversidad en el ruido

  El uso constante de np.random.normal(0,1) puede llevar al generador a quedarse atascado. Técnicas como truncated normal o Interpolación de ruido podrían ayudar.

4. Demasiadas epochs sin perturbación

  Entrenar hasta 40,000 epochs sin aplicar técnicas como Dropout dinámico, noise injection, o cambios de optimizador puede permitir que el modelo colapse.

**Ajustes Recomendados para Recuperar Capacidad**

1. Técnica	Descripción	Impacto Esperado

  a. Restart Training with Checkpoint	Volver a un modelo anterior (ej: epoch 22000 o 25000) donde aún había diversidad	Recuperar capacidad generativa

2. Truncated Noise	Usa np.random.truncated_normal para ruido más diverso	Aumenta la diversidad de outputs

3. One-sided Label Smoothing	valid = 0.9 y fake = 0.1	Hace el discriminador menos confiado

4. Batch Discrimination

  Se podria agrega una capa que fuerza diversidad en el generador	Evita modo colapso

5. Feature Matching Loss	En vez de solo engañar al discriminador, obliga al generador a igualar features	Mejora estabilidad

6. Cambiar optimizador	Alternar entre Adam, RMSprop o usar learning rate decay	Mejora dinámica de entrenamiento

**Conclusión**

El modelo comenzó bien con señales de diversidad tonal, pero a medida que avanzó, se colapsó en patrones poco diversos y binarios. El cambio al enfoque manual fue acertado, pero ahora es momento de aplicar técnicas avanzadas anti-colapso o reiniciar desde un checkpoint anterior.

**Decisión Final**

Tras evaluar la calidad visual, estabilidad y control del flujo de trabajo, decidí continuar el entrenamiento con el método manual inicial, que aunque más básico, ofrecía mayor control sobre las entradas, las formas, y la lógica de entrenamiento. Esta decisión me permitió extender el modelo hasta 40,000+ épocas y observar una evolución más progresiva, controlada y reproducible en los resultados.

# **Respuestas a Preguntas Teóricas Unidad 2**

**Parte 1: Implementación de una CNN para CIFAR-100**

**¿Necesitaré una arquitectura igual a la utilizada para CIFAR-10?**

No exactamente. Aunque CIFAR-10 y CIFAR-100 comparten estructura (32x32 imágenes a color), la complejidad del problema es distinta. CIFAR-100 tiene 10 veces más clases, lo cual implica una mayor complejidad y demanda una arquitectura más profunda o con mayor capacidad representacional. Por tanto, si bien puede tomarse como base, la arquitectura de CIFAR-10 requiere modificaciones o mejoras para adaptarse a CIFAR-100.

**¿Será un problema más o menos complicado para la red neuronal?**

Más complicado. Aumentar la cantidad de clases de 10 a 100 multiplica la dificultad de discriminación entre ellas, sobre todo porque algunas clases son visualmente similares. Esto puede aumentar el riesgo de overfitting y hacer más lenta la convergencia.

**¿Debemos interpretar las métricas de la misma forma que con CIFAR-10?**

No exactamente. Aunque las métricas como accuracy se interpretan de la misma forma técnica, debemos ser más flexibles al evaluar resultados en CIFAR-100. Un accuracy del 30–40% en CIFAR-100 puede considerarse un resultado aceptable en una arquitectura sencilla, mientras que en CIFAR-10 se esperaría >80%.

**¿Qué es lo mínimo que se debe cambiar para adaptar una red de CIFAR-10 a CIFAR-100?**

Lo mínimo es cambiar la capa de salida de 10 neuronas a 100, para adecuarse al nuevo número de clases. También habría que ajustar la función de pérdida para asegurar compatibilidad (por ejemplo, usar categorical_crossentropy si las etiquetas están one-hot encoded).

**Parte 2: Implementación de una CGAN para Fashion MNIST**

**¿Habría que hacer algún cambio obligatorio respecto a la red utilizada con MNIST?**

Sí. Aunque la resolución de las imágenes es la misma (28x28), los patrones visuales en Fashion MNIST son más complejos (ropa, texturas) comparado con los dígitos simples de MNIST. Esto exige una red generadora y discriminadora más capaz de capturar relaciones espaciales complejas. Además, es necesario ajustar la entrada condicional para que las clases de Fashion MNIST se integren correctamente en ambas redes (generador y discriminador).

**¿Deberán ser las redes más complejas, igual o menos complejas que en el caso de MNIST?**

Deben ser más complejas. Fashion MNIST presenta mayor variabilidad intra-clase y formas más difíciles de generar de manera creíble. Esto requiere que tanto el generador como el discriminador tengan más capas, o al menos capas con más filtros/unidades, para poder modelar la complejidad de las prendas de vestir. También podrían necesitarse técnicas como BatchNorm, Dropout, y mayor capacidad en los embeddings de clase.

**Justificación:**

1. Fashion MNIST contiene detalles finos que requieren mayor capacidad de representación.

2. El generador necesita mayor potencia para crear bordes y contornos realistas.

3. El discriminador necesita ser más sensible para distinguir imágenes sintéticas de reales con base en patrones sutiles.
"""