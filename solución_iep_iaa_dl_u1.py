# -*- coding: utf-8 -*-
"""Solución IEP-IAA-DL-u1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8J8A5O6M2nNMSoEZEsoJRjR_ZZKXgRi

**Ejercicio 1: Regresión con datos reales**

Usaré el dataset Ames Housing, que es más completo y moderno que el de California Housing. Este dataset incluye información detallada sobre viviendas en Ames, Iowa, y permite predecir el precio de venta.

**Tarea:** Predecir el precio de venta (SalePrice)

**Evaluación:** RMSE (Root Mean Squared Error)

**Framework:** Keras + TensorFlow

**Enfoque:** Probar arquitecturas con regularización (L2, Dropout), normalización de inputs y diferentes funciones de activación
"""

# Paso 1: Cargar y preprocesar el dataset de Ames Housing
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2

# Ruta proporcionada por el usuario
file_path = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u1/AmesHousing.csv"

# Cargar dataset desde Google Drive
data = pd.read_csv(file_path)

# Mostrar las primeras filas para verificar
data.head()

# Seleccionamos columnas numéricas para simplificar
numerical_data = data.select_dtypes(include=[np.number])

# Eliminamos columnas con muchos valores nulos
numerical_data = numerical_data.dropna(axis=1, thresh=len(numerical_data)*0.9)

# Eliminamos filas con valores nulos remanentes
numerical_data = numerical_data.dropna()

# Separar features y target
X = numerical_data.drop(columns="SalePrice")
y = numerical_data["SalePrice"]

# Dividir en train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Estandarizar los datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Mostramos la forma de los datos listos
X_train.shape, X_train_scaled.shape, y_train.shape

# Definir el modelo secuencial
model = Sequential([
    Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    Dense(1, activation='linear')  # Capa de salida para regresión
])

# Compilar el modelo
model.compile(optimizer='adam', loss='mean_squared_error')

# Entrenar el modelo
history = model.fit(
    X_train_scaled,
    y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=32,
    verbose=1
)

# Evaluar el modelo
y_pred = model.predict(X_test_scaled)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("RMSE en el conjunto de prueba:", rmse)

import matplotlib.pyplot as plt

# Visualizar la pérdida en entrenamiento y validación
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Entrenamiento', linewidth=2)
plt.plot(history.history['val_loss'], label='Validación', linewidth=2)
plt.xlabel('Época')
plt.ylabel('MSE (Error Cuadrático Medio)')
plt.title('Curvas de entrenamiento y validación')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""**Conclusión**

En este ejercicio construimos un modelo de red neuronal profunda con Keras para predecir el precio de venta de viviendas (SalePrice), utilizando el conjunto de datos Ames Housing.

**Objetivo:** Resolver un problema de regresión y evaluar el modelo utilizando el RMSE (Root Mean Squared Error) como métrica principal.

**Resultados**

**Arquitectura del modelo:**

2 capas ocultas densas con 64 y 32 neuronas.

Activación ReLU.

Regularización L2 y capa de Dropout (0.2).

**Función de pérdida:** mean_squared_error

**Optimizador:** Adam

RMSE en el conjunto de prueba: $54,340.11

**Análisis**

La visualización del error durante el entrenamiento y la validación muestra una convergencia estable y paralela, lo cual es una excelente señal de que:

No hubo sobreajuste (overfitting).

No se produjo subajuste (underfitting).

El modelo generaliza bien a datos no vistos.

Dado este comportamiento, no fue necesario implementar técnicas adicionales como batch normalization o ajustar el dropout.

**Conclusiones**

1. El modelo logró una buena capacidad predictiva utilizando únicamente características numéricas, sin ingeniería de features.

2. La estandarización de los datos y la regularización L2 fueron claves para estabilizar el aprendizaje.

3. La arquitectura propuesta es una base robusta para producción, aunque podría optimizarse aún más con ingeniería de variables y validación cruzada.

**Ejercicio 2: Clasificación de imágenes con Fashion MNIST**

Usaré Fashion MNIST, como en el ejemplo, ya que es ligero, conocido y se ajusta bien a redes densas sin requerir arquitecturas convolucionales.

**Tarea:** Clasificar imágenes en 10 tipos de ropa

**Evaluación:** Accuracy

**Framework:** Keras + TensorFlow

**Enfoque:** Probar diferentes tamaños de red, Dropout, Batch Normalization y optimizadores
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import StandardScaler

# Cargar CSVs desde Drive
train_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u1/Fashion dataset/fashion-mnist_train.csv")
test_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u1/Fashion dataset/fashion-mnist_test.csv")

# Separar features y etiquetas
X_train = train_df.drop(columns="label").values
y_train = train_df["label"].values

X_test = test_df.drop(columns="label").values
y_test = test_df["label"].values

# Normalizar valores de píxeles entre 0 y 1
X_train = X_train / 255.0
X_test = X_test / 255.0

# One-hot encoding de las etiquetas
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Mostrar forma de los datos
print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# Definir el modelo
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),
    Dense(10, activation='softmax')
])

# Compilar el modelo
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Entrenar el modelo
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=64,
    validation_split=0.2,
    verbose=1
)

# Evaluar en el conjunto de prueba
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Pérdida en test:", test_loss)
print("Precisión en test:", test_accuracy)

plt.plot(history.history['accuracy'], label='Entrenamiento')
plt.plot(history.history['val_accuracy'], label='Validación')
plt.title('Precisión durante el entrenamiento')
plt.xlabel('Época')
plt.ylabel('Precisión')
plt.legend()
plt.grid()
plt.show()

"""**Resultados**

**Precisión en test:** 88.7%

**Pérdida (loss) en test**: 0.315

**La curva de precisión muestra:**

Aprendizaje estable.

Ligero gap entre entrenamiento y validación desde la época 6, indicando posible inicio de sobreajuste leve.

El modelo generaliza bien sin regularización extra.

**Conclusión**

El modelo alcanza un buen rendimiento con una arquitectura sencilla y sin necesidad de técnicas adicionales como Dropout o BatchNormalization. La precisión en validación y test es estable y competitiva para un modelo de red neuronal densa en Fashion MNIST.

Por tanto, no es necesario ajustar más variantes en esta etapa.
"""