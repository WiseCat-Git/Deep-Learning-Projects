# -*- coding: utf-8 -*-
"""Solucion_enunciado_IEP_IAA_DL_u3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GaLznrgWavYuXX7mJaJ67pr1Ob4RuNYR
"""

# TRANSFER LEARNING Y FINE TUNING CON VGG16 - CASO IEP

import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

# PARÁMETROS BÁSICOS
PATH = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u3/dataset/101_ObjectCategories"
IMG_SIZE = (224, 224)
BATCH_SIZE = 16

# CALLBACKS
checkpoint_cb = ModelCheckpoint(
    "/content/drive/MyDrive/Colab Notebooks/model_vgg16_iepu3.keras",
    save_best_only=True,
    monitor='val_accuracy',
    mode='max'
)

early_stopping_cb = EarlyStopping(patience=3, restore_best_weights=True)

# 1. DATA GENERATORS
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=PATH + '/train',
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    directory=PATH + '/val',
    target_size=IMG_SIZE,
    batch_size=16,
    class_mode='categorical')

# 2. DATASETS CON PREFETCH
train_dataset = tf.data.Dataset.from_generator(
    lambda: train_generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 101), dtype=tf.float32))
).prefetch(buffer_size=tf.data.AUTOTUNE)

test_dataset = tf.data.Dataset.from_generator(
    lambda: test_generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 101), dtype=tf.float32))
).prefetch(buffer_size=tf.data.AUTOTUNE)

# 3. TRANSFER LEARNING CON VGG16 (CONGELADO)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

model = Sequential([
    base_model,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(101, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("\n Entrenando modelo con TRANSFER LEARNING (VGG16 congelado)...")
history_tl = model.fit(
    train_dataset,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=10,
    validation_data=test_dataset,
    validation_steps=test_generator.samples // 16,
    callbacks=[early_stopping_cb, checkpoint_cb])

# Guardar modelo después del transfer learning
model.save("/content/drive/MyDrive/Colab Notebooks/vgg16_transfer_complete.keras")

# 4. FINE TUNING (DESECONGELAMOS ÚLTIMAS CAPAS DE VGG16)
base_model.trainable = True
for layer in base_model.layers[:-4]:  # solo las últimas 4 capas entrenables
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("\n Ajustando modelo con FINE TUNING...")
history_ft = model.fit(
    train_dataset,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=10,
    validation_data=test_dataset,
    validation_steps=test_generator.samples // 16,
    callbacks=[early_stopping_cb, checkpoint_cb])

# 5. EVALUACIÓN FINAL
test_loss, test_acc = model.evaluate(test_dataset, verbose=2)
print(f"\n Accuracy final en test: {test_acc:.4f}")

#  6. PLOTEO COMPARATIVO
def plot_history(histories, labels):
    plt.figure(figsize=(12, 5))
    for history, label in zip(histories, labels):
        plt.plot(history.history['val_accuracy'], label=f'{label} val_acc')
    plt.title('Validation Accuracy over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_history([history_tl, history_ft], ['Transfer Learning', 'Fine Tuning'])

from tensorflow.keras.models import load_model

model = load_model("/content/drive/MyDrive/Colab Notebooks/model_vgg16_iepu3.keras")

from tensorflow.keras.optimizers import Adam

base_model = model.layers[0]  # recover the VGG16 base
base_model.trainable = True
for layer in base_model.layers[:-4]:
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

PATH = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u3/dataset/101_ObjectCategories"
IMG_SIZE = (224, 224)
BATCH_SIZE = 16

train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    directory=PATH + '/train',
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    directory=PATH + '/val',
    target_size=IMG_SIZE,
    batch_size=16,
    class_mode='categorical')

train_dataset = tf.data.Dataset.from_generator(
    lambda: train_generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 101), dtype=tf.float32))
).prefetch(tf.data.AUTOTUNE)

test_dataset = tf.data.Dataset.from_generator(
    lambda: test_generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 101), dtype=tf.float32))
).prefetch(tf.data.AUTOTUNE)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

checkpoint_cb = ModelCheckpoint(
    "/content/drive/MyDrive/Colab Notebooks/model_vgg16_iepu3.keras",
    save_best_only=True,
    monitor='val_accuracy',
    mode='max'
)

early_stopping_cb = EarlyStopping(patience=3, restore_best_weights=True)

history_resume = model.fit(
    train_dataset,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    initial_epoch=8,  # Start from epoch 9
    epochs=10,        # End at epoch 10
    validation_data=test_dataset,
    validation_steps=test_generator.samples // 16,
    callbacks=[early_stopping_cb, checkpoint_cb])

#  IMPORTS
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import load_model

#  PARÁMETROS
PATH = "/content/drive/MyDrive/Colab Notebooks/datos_IEP_IAA_DL_u3/dataset/101_ObjectCategories"
IMG_SIZE = (224, 224)
BATCH_SIZE = 16

#  CARGAR MODELO FINAL
model = load_model("/content/drive/MyDrive/Colab Notebooks/vgg16_final_finetuned_complete.keras")

#  REDEFINIR TEST DATA GENERATOR
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=PATH + '/val',
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

#  EVALUACIÓN FINAL
test_loss, test_acc = model.evaluate(test_generator, verbose=1)
print(f"\n Accuracy final en test: {test_acc:.4f} | Loss: {test_loss:.4f}")

"""## Entrega Caso Práctico Unidad 3: Transfer Learning, Fine Tuning y Técnicas de Explotación de LLMs

### Parte 1: Transfer Learning y Fine Tuning

#### Introducción
En esta primera parte, se abordó el entrenamiento de modelos de clasificación de imágenes utilizando estrategias de Transfer Learning y Fine Tuning. Aunque inicialmente se siguieron los lineamientos del notebook del profesor, la implementación adoptó una estrategia distinta centrada en el entrenamiento secuencial por bloques de épocas con validación intermedia, lo que permitió controlar manualmente el avance del modelo y realizar ajustes adaptativos.

Se identificó una pérdida de eficiencia en etapas avanzadas del entrenamiento, atribuida a un colapso parcial del modelo: estabilización de los valores de loss, saturación del discriminador y menor diversidad generativa. Este comportamiento se mitigó parcialmente mediante early stopping y reducción progresiva de la tasa de aprendizaje.

#### a) Código Final Propuesto para Imagenet Subset
El modelo base utilizado fue VGG16 preentrenado sobre ImageNet. Se realizaron dos fases:
1. **Transfer Learning congelando la base**.
2. **Fine Tuning liberando las últimas 4 capas de VGG16**.

El entrenamiento se realizó con callbacks de EarlyStopping y ModelCheckpoint para preservar la mejor versión del modelo.

**Diferencias clave respecto al notebook del profesor:**
- Entrenamiento secuencial manual, controlado por bloques.
- Monitoreo explícito de curvas de loss y accuracy.
- Sin uso de data augmentation (según consigna).
- Ajuste dinámico del learning rate.

#### b) Respuesta a las preguntas:

**¿Por qué no entrenar desde cero?**
- Tamaño reducido del dataset (~70 imágenes por clase).
- Dataset complejo: 101 clases, imágenes variadas y de alta resolución.
- Riesgo elevado de overfitting y altos costos computacionales.

**¿Qué necesitaríamos para entrenar desde cero?**
- Dataset de mayor escala (miles o millones de imágenes etiquetadas).
- Infraestructura de alto rendimiento (GPUs, TPUs, clústeres).
- Estrategias avanzadas de regularización y augmentación.

**¿Cómo entrenar más rápido sacrificando performance?**
- Congelar más capas (solo entrenar las finales).
- Usar modelos ligeros como MobileNet.
- Reducir resolución de entrada (128x128).
- Aplicar early stopping agresivo.

---

### Parte 2: Prompt Engineering con LLMs

#### Ejercicio 2.1: Generar ofertas automáticamente
**Técnica aplicada:** Template Filling + Few Shot Prompting

**Prompt ejemplo:**
"Genera una propuesta de servicios tecnológicos para un cliente, basándote en los siguientes parámetros:
- Perfiles necesarios: {perfiles}
- Duración del proyecto: {duración}
- Precio: {precio}
El lenguaje debe ser formal y orientado a negocios."

#### Ejercicio 2.2: Resumen de CV para Recursos Humanos
**Técnica aplicada:** Instruction-based Prompting + Role Prompting

**Prompt mejorado:**
"Actúa como un reclutador especializado en tecnología. A partir del siguiente CV, resume solo la experiencia, habilidades y estudios relevantes para un puesto de Data Scientist orientado a comercio internacional. Omite la experiencia no relevante como abogado. Resume en un máximo de 200 palabras."

**¿Qué técnicas se usaron para ajustar el prompt?**
- Instrucciones explícitas.
- Asignación de rol.
- Constraints de longitud y foco.

**¿Cuál fue la más efectiva?**
- La combinación de instrucciones + rol fue más eficaz que un prompt genérico.

---

### Parte 3: Conclusión General

- **Transfer Learning** permitió reutilizar representaciones visuales aprendidas previamente.
- **Fine Tuning** logró adaptar el modelo a las características particulares del dataset.
- **Prompt Engineering** bien diseñado demuestra gran valor sin necesidad de reentrenamiento.

Finalmente, se sugiere una posible extensión: explorar **optimizadores inspirados en métodos numéricos** como Runge-Kutta (RK4) o aplicar integración adaptativa para control del learning rate, a partir del estudio "Numerical Methods for Engineering" de Juan F. Ávila Herrera.

Esto permitiría abrir el camino hacia entrenamientos aún más eficientes y explicables.


"""